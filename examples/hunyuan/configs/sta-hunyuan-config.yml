model_name: hunyuan
world_size: 1

patchify:
  is_enabled: true

mlp:
  is_enabled: false

attn:
  # Turn on debug to print sparsity levels
  # debug: true
  is_enabled: false

  # How much probability mass of the attention matrix to cover
  # coverage: 0.7
  top_keys: 0
  random_keys: 0

  # Number of local voxels to use for static local attention
  local_voxels: 5
  full_attn_from_3d_tail: true
  full_attn_to_3d_tail: true
  delta_cache: false

  # Local 1D window in raster order for DiTFastAttn
  local_1d_window: 0

  first_n_dense_layers: 2
  recompute_mask: true
  should_compress_indices: true

  # If schedule is not None, will override full_step_every
  full_step_schedule: !!set
    ? 0
    ? 1
    ? 10
    ? 40
  # full_step_every: 3

  # Do not change below this line -- kernel-specific business
  pad_qkv_before_kernel: true

offloading:
  mlp.out_cache: false
  attn.out_cache: true
  attn.indices: true
  text_encoders: true

tea_cache:
  is_enabled: false
  threshold: 0.1
  debug: true

token_cache:
  is_enabled: false
  cache_ratio: 0.85
  full_every: 3

step_caching:
  is_enabled: true
  # Feel free to play with this -- it's pretty stable across different schedules!
  skip_step_schedule: !!set
    ? 7
    ? 11
    ? 13
    ? 14
    ? 15
    ? 17
    ? 18
    ? 19
    ? 21
    ? 22
    ? 23
    ? 25
    ? 26
    ? 27
    ? 29
    ? 31
    ? 33
    ? 34
    ? 35
    ? 37
    ? 38
    ? 39
    ? 41
    ? 42
    ? 43
